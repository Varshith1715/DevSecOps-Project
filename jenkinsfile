pipeline {
  agent any

  parameters {
    string(name: 'IMAGE_TAG', defaultValue: 'latest', description: 'Docker image tag')
    string(name: 'LOAD_BALANCER_IP', defaultValue: '', description: 'Public IP or DNS of LoadBalancer service')
    string(name: 'RELEASE_NAME', defaultValue: 'taxi-booking', description: 'Helm release name')
  }

  environment {
    IMAGE_NAME = "taxi-booking"
    DOCKERHUB_CREDENTIALS_ID = 'dockerhub'
    DOCKERHUB_USER = 'varshith999'
    KUBE_NAMESPACE = "default"
    CHART_PATH = "${env.WORKSPACE}/terraform/web-app-chart"
    KUBE_CONFIG_PATH = "/var/lib/jenkins/.kube/config"
  }

  stages {
    stage('Checkout Code') {
      steps {
        checkout scm
      }
    }

    stage('Verify Workspace') {
      steps {
        sh """
          echo "Verifying the workspace content..."
          ls -al ${env.WORKSPACE}/terraform
        """
      }
    }

    stage('Static Code Analysis - Semgrep') {
      steps {
        sh '''
          echo "Running Semgrep..."
          docker run --rm -v $(pwd):/src returntocorp/semgrep semgrep scan --config=auto /src
        '''
      }
    }

    stage('Build Docker Image from Existing WAR') {
      steps {
        script {
          def imageExists = sh(
            script: "docker images -q ${DOCKERHUB_USER}/${IMAGE_NAME}:${params.IMAGE_TAG}",
            returnStdout: true
          ).trim()

          if (imageExists) {
            echo "✅ Docker image ${DOCKERHUB_USER}/${IMAGE_NAME}:${params.IMAGE_TAG} already exists locally. Skipping build."
          } else {
            sh """
              echo "Using existing WAR file..."
              WAR_FILE="taxi-booking/target/taxi-booking-1.0.1.war"
              echo "Building Docker image..."
              cp \$WAR_FILE ./ || { echo "WAR file not found!"; exit 1; }
              docker build -t ${DOCKERHUB_USER}/${IMAGE_NAME}:${params.IMAGE_TAG} .
            """
          }
        }
      }
    }

    stage('Update Trivy DB') {
      steps {
        sh """
          echo "Updating Trivy vulnerability database..."
          docker run --rm aquasec/trivy image --download-db-only
        """
      }
    }

    stage('Scan Docker Image - Trivy') {
      steps {
        script {
          def imageId = sh(script: "docker images -q ${DOCKERHUB_USER}/${IMAGE_NAME}:${params.IMAGE_TAG}", returnStdout: true).trim()
          echo "Scanning image ID: ${imageId}"
          sh """
            echo "Running Trivy scan (non-blocking)..."
            docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
              aquasec/trivy image --severity HIGH,CRITICAL --exit-code 0 ${imageId} || echo "Trivy scan failed, continuing..."
          """
        }
      }
    }

    stage('Push to DockerHub') {
      steps {
        withCredentials([usernamePassword(credentialsId: "${DOCKERHUB_CREDENTIALS_ID}", passwordVariable: 'DOCKER_PASSWORD', usernameVariable: 'DOCKER_USERNAME')]) {
          sh """
            echo "\${DOCKER_PASSWORD}" | docker login -u "\${DOCKER_USERNAME}" --password-stdin
            docker push \${DOCKER_USERNAME}/\${IMAGE_NAME}:${params.IMAGE_TAG}
          """
        }
      }
    }

    stage('Deploy to EKS using Helm') {
      steps {
        script {
          def chartExists = fileExists("${CHART_PATH}/Chart.yaml")
          if (!chartExists) {
            error "Helm chart directory not found at ${CHART_PATH}"
          }

          sh """
            echo "Deploying to EKS using Helm..."
            helm upgrade --install ${params.RELEASE_NAME} ${CHART_PATH} \
              --set image.repository=${DOCKERHUB_USER}/${IMAGE_NAME} \
              --set image.tag=${params.IMAGE_TAG} \
              --namespace ${KUBE_NAMESPACE} \
              --kubeconfig ${KUBE_CONFIG_PATH}
          """
        }
      }
    }

    stage('Dynamic App Security Testing - OWASP ZAP') {
      steps {
        script {
          def zapReportPath = "${env.WORKSPACE}/zap-report.html"
          def zapReportXmlPath = "${env.WORKSPACE}/zap-report.xml"
          echo "Running OWASP ZAP baseline scan..."

          // Ensure directory permissions for mounted volumes
          sh """
            echo "Ensuring correct directory permissions for ZAP scan..."
            sudo chmod -R 777 ${env.WORKSPACE}
          """

          // Replace NodePort with LoadBalancer IP or DNS
          def targetUrl = "http://${params.LOAD_BALANCER_IP}:8080"
          
          sh """
            docker run -v ${env.WORKSPACE}:/zap/wrk/:rw -t ghcr.io/zaproxy/zaproxy zap-baseline.py \
              -t ${targetUrl} -g gen.conf -r ${zapReportPath} -x ${zapReportXmlPath} \
              -l FAIL --exit-zero || exit 1
          """
        }
      }
    }

    stage('Install OPA Gatekeeper') {
      steps {
        sh """
          echo "Installing OPA Gatekeeper if not already present..."
          if ! kubectl get pods -n gatekeeper-system --kubeconfig ${KUBE_CONFIG_PATH} &> /dev/null; then
            kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.14/deploy/gatekeeper.yaml --kubeconfig ${KUBE_CONFIG_PATH}
            kubectl rollout status deployment/gatekeeper-controller-manager -n gatekeeper-system --kubeconfig ${KUBE_CONFIG_PATH}
          else
            echo "OPA Gatekeeper is already installed."
          fi
        """
      }
    }

    stage('Apply OPA Constraints & Templates') {
      steps {
        sh """
          echo "Applying OPA policies..."
          kubectl apply -f opa/policies/privileged-template.yaml --kubeconfig ${KUBE_CONFIG_PATH}
          kubectl apply -f opa/policies/privileged-constraint.yaml --kubeconfig ${KUBE_CONFIG_PATH}
          kubectl apply -f opa/policies/trusted-registry-template.yaml --kubeconfig ${KUBE_CONFIG_PATH}
          kubectl apply -f opa/policies/trusted-registry-constraint.yaml --kubeconfig ${KUBE_CONFIG_PATH}
        """
      }
    }

    stage('OPA Gatekeeper Policy Check') {
      steps {
        sh """
          echo "Checking OPA Gatekeeper constraint violations..."
          VIOLATIONS=\$(kubectl get constrainttemplates -o json --kubeconfig ${KUBE_CONFIG_PATH} | jq '.items[] | .status?.totalViolations' | grep -v null | awk '\$1 > 0')
          if [ -n "\$VIOLATIONS" ]; then
            echo "❌ Violations found by OPA Gatekeeper!"
            exit 1
          else
            echo "✅ No OPA policy violations detected."
          fi
        """
      }
    }
  }

  post {
    success {
      echo "✅ Pipeline completed successfully!"
    }
    failure {
      echo "❌ Pipeline failed. Check the logs for details."
    }
  }
}
